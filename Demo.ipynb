{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animal to Dino Transformer\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\issac\\AppData\\Local\\Temp\\ipykernel_30608\\3918576605.py:20: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  animal_df = pd.read_csv('Datasets\\Animal-Info.csv')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt     # plotting\n",
    "from matplotlib import cm           # colormaps\n",
    "import pandas as pd                 # data processing, CSV file I/O\n",
    "import numpy as np                  # linear algebra\n",
    "from tqdm import tqdm               # fancy progress bars\n",
    "\n",
    "#suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load in animal data\n",
    "animal_df = pd.read_csv('Datasets\\Animal-Info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import zipfile as zf\n",
    "\n",
    "images1 = zf.ZipFile(\"Images/images1.zip\", \"r\")\n",
    "images1.extractall('images1')\n",
    "\n",
    "images2 = zf.ZipFile(\"Images/images2.zip\", \"r\")\n",
    "images2.extractall('images2')\n",
    "\n",
    "images1.close()\n",
    "images2.close()\n",
    "\n",
    "dinosaur = pd.read_csv('Datasets/jurassicparkwithweights.csv')\n",
    "distances = pd.read_csv('Datasets/distances.csv')\n",
    "animal = pd.read_csv('Datasets/Clean-Animal-Info.csv')\n",
    "\n",
    "animals = pd.read_csv(\"Datasets/Animal-Info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Cleaning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Cleaning\n",
    "def remove_na(df):\n",
    "    temp = df\n",
    "    for column in temp.columns:\n",
    "        temp = temp[(temp[column] != 'Varies') & (temp[column] != 'Not Applicable')]\n",
    "    return temp\n",
    "\n",
    "def clean_column(item):\n",
    "    item = item.replace(',', '')\n",
    "    if item[-1] == 'm':\n",
    "        split = item.split()\n",
    "        item = str(float(split[2][:-1])*100)\n",
    "    if item.count('tons') > 0:\n",
    "        split = item.split()\n",
    "        item = str(float(split[2])*1000)\n",
    "        \n",
    "    out = 0\n",
    "    split = item.replace(' ', '-').split('-')\n",
    "    if item.count('-') > 0:\n",
    "        out = (float(split[0]) + float(split[1]))/2\n",
    "    if item.count('Up to') > 0:\n",
    "        out = float(item.split()[-1])/2\n",
    "    if split[-1] == 'months':\n",
    "        out *= 30\n",
    "    if split[-1] == 'weeks':\n",
    "        out *= 7\n",
    "    only_nums = lambda x: ''.join([i for i in x if i.isnumeric() or i == '.'])\n",
    "    \n",
    "    return out if out else float(only_nums(item))\n",
    "\n",
    "def z_score(x, based_on=None):\n",
    "    if based_on is None: based_on = x\n",
    "    return (x - np.mean(based_on))/np.std(based_on)\n",
    "\n",
    "def normalize(nums, scale=np.log):\n",
    "    nums = scale(nums)\n",
    "    return z_score(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Cleaning\n",
    "\n",
    "clean_animals = remove_na(animal_df)\n",
    "columns_to_clean = ['Height (cm)', 'Weight (kg)', 'Lifespan (years)', \n",
    "                    'Average Speed (km/h)', 'Top Speed (km/h)', \n",
    "                    'Gestation Period (days)']\n",
    "\n",
    "for column in columns_to_clean:\n",
    "    category = ' '.join(column.split()[:-1])\n",
    "    clean_animals = clean_animals.assign(**{category: clean_animals[column].apply(clean_column)})\n",
    "    clean_animals = clean_animals.assign(**{f'Normal {category}': normalize(clean_animals[category])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "inputs = ['Normal Height', 'Normal Weight']\n",
    "outputs = ['Lifespan', 'Average Speed', 'Top Speed', 'Gestation Period']\n",
    "input_tensors = torch.tensor(np.array(clean_animals[inputs]), dtype=torch.float)\n",
    "output_tensors = torch.tensor(np.array(clean_animals[outputs]), dtype=torch.float)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input_tensors, output_tensors, test_size=0.1)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LAYERS = [len(inputs), 10, 10, 10, len(outputs)]\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input = self.inputs[idx]\n",
    "        output = self.outputs[idx]\n",
    "        return input, output\n",
    "    \n",
    "train_ds = CustomDataset(X_train, Y_train)\n",
    "test_ds = CustomDataset(X_test, Y_test)\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, dims, stack=None):\n",
    "        super().__init__()\n",
    "        if stack:\n",
    "            self.stack = stack\n",
    "        else:\n",
    "            self.stack = nn.Sequential(nn.Dropout(p=0.1))\n",
    "            for i in range(len(dims)-1):\n",
    "                self.stack.append(nn.Linear(dims[i], dims[i+1]))\n",
    "                if i != len(dims)-2:\n",
    "                    self.stack.append(nn.ReLU())\n",
    "                    self.stack.append(nn.Dropout(p=0.05))\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, len(inputs))\n",
    "        return self.stack(x)\n",
    "    \n",
    "def run(X, Y, model, loss):\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    y_pred = model(X)\n",
    "    y_pred, Y = y_pred.cpu(), Y.cpu()\n",
    "    error = loss(y_pred, Y)\n",
    "    return error\n",
    "\n",
    "def train_epoch(model, optimizer, loss):\n",
    "    model.train()\n",
    "    errors = []\n",
    "\n",
    "    for X, Y in train_dl:\n",
    "        error = run(X, Y, model, loss)\n",
    "        errors.append(error.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(errors)\n",
    "\n",
    "def evaluate(model, loss):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_dl:\n",
    "            error = run(X, Y, model, loss)\n",
    "            errors.append(error.item())\n",
    "    \n",
    "    return np.mean(errors)\n",
    "\n",
    "def show_plot(model, x, y, z, show_pred=False, show_terrain=False, show_all=False):\n",
    "    fig = plt.figure()\n",
    "    norm_x, norm_y, norm_z = f'Normal {x}', f'Normal {y}', f'Normal {z}'\n",
    "    \n",
    "    loop_len = range(len(outputs)) if show_all else [outputs.index(z)]\n",
    "    for i in loop_len:\n",
    "        if show_all: ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "        else: ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        if show_pred:\n",
    "            if show_terrain:\n",
    "                X = np.arange(-5, 5, 0.25)\n",
    "                Y = np.arange(-5, 5, 0.25)\n",
    "                grid_X, grid_Y = np.meshgrid(X, Y)\n",
    "                grid_X = torch.tensor(grid_X, dtype=torch.float)\n",
    "                grid_Y = torch.tensor(grid_Y, dtype=torch.float)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if show_terrain:\n",
    "                    Z = model(torch.stack((grid_X, grid_Y), dim=2).reshape(-1, 2)).cpu().numpy()\n",
    "                    ax.plot_surface(grid_X, grid_Y, Z[:, i].reshape(len(X), len(Y)), cmap=cm.coolwarm)\n",
    "                else:\n",
    "                    inputs = torch.tensor(np.array(clean_animals[[norm_x, norm_y]]), dtype=torch.float, device=device)\n",
    "                    Z = model(inputs).cpu().numpy()\n",
    "                    ax.scatter(clean_animals[norm_x], clean_animals[norm_y], Z[:, i], marker='o')\n",
    "            \n",
    "        ax.scatter(clean_animals[norm_x], clean_animals[norm_y], clean_animals[z], marker='^', color='orange')\n",
    "        ax.set_xlabel(x)\n",
    "        ax.set_ylabel(y)\n",
    "        ax.set_zlabel(z)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def Train(learn_rate=0.001, epochs=30, plot_info=True):\n",
    "    network = Network(LAYERS)\n",
    "    network = network.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(network.parameters(), lr = learn_rate, weight_decay=1e-5)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for epoch in (bar := tqdm(range(epochs))):\n",
    "        batch_error = train_epoch(network, optimizer, loss_fn)\n",
    "        train_errors.append(batch_error)\n",
    "\n",
    "        test_error = evaluate(network, loss_fn)\n",
    "        test_errors.append(test_error)\n",
    "        \n",
    "        bar.set_description(f'Error: {test_error:.2f}')\n",
    "\n",
    "    if plot_info:\n",
    "        fig, error = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "        error.plot(train_errors)\n",
    "        error.plot(test_errors)\n",
    "        error.legend(['Train', 'Test'])\n",
    "        error.set_xlabel('Epochs')\n",
    "        error.set_ylabel('Error')\n",
    "        error.set_title(f'Train Error: {train_errors[-1]:.2f} Test Error: {test_errors[-1]:.2f}')\n",
    "\n",
    "    return network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: 4216.18: 100%|██████████| 100/100 [00:01<00:00, 57.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Final ml stuff\n",
    "network = Train(epochs=100, plot_info=False)\n",
    "\n",
    "#show_plot(network, 'Height', 'Weight', 'Lifespan', show_pred=True)\n",
    "\n",
    "dinos = pd.read_csv('Datasets/jurassicparkwithweights.csv')\n",
    "clean_dinos = dinos.drop(columns='Unnamed: 0').set_index('name')\n",
    "\n",
    "def pred_stat(dino, stat, model):\n",
    "    METERS2CM = 100\n",
    "    length2height = lambda length: 0.53*length - 0.73   # From Regression\n",
    "    \n",
    "    info = clean_dinos[['length', 'weight']].loc[dino]\n",
    "    info[0] = length2height(info[0])\n",
    "    \n",
    "    norm_height = z_score(info[0]*METERS2CM, based_on=clean_animals['Height'])\n",
    "    norm_weight = z_score(info[1], based_on=clean_animals['Weight'])\n",
    "    inputs = torch.tensor([norm_height, norm_weight], dtype=torch.float)\n",
    "    predictions = model(inputs).squeeze()\n",
    "    return predictions[outputs.index(stat)].item()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Calculator Cleaning and Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean distances DataFrame\n",
    "def clean_whitespace(country):\n",
    "    return country[1:]\n",
    "\n",
    "distances = distances.assign(Country2=distances['Country2'].apply(clean_whitespace))\n",
    "distances = distances.set_index('Country1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dinosaur DataFrame\n",
    "def format_diet(diet): \n",
    "    conversion = {\n",
    "        \"herbivorous\": 'Herbivore',\n",
    "        \"carnivorous\": 'Carnivore',\n",
    "        \"omnivorous\": 'Omnivore'\n",
    "    }\n",
    "    if diet in conversion:\n",
    "        return conversion[diet]\n",
    "    return None\n",
    "\n",
    "def format_height(height):\n",
    "    height = str(height)\n",
    "    height = height.strip('m')\n",
    "    return .53 * float(height)\n",
    "\n",
    "def clean_countries(country):\n",
    "    if country == 'USA':\n",
    "        return \"United States\"\n",
    "    elif country == 'North Africa':\n",
    "        return \"Algeria\"\n",
    "    elif country == 'Wales':\n",
    "        return \"United Kingdom\"\n",
    "    else:\n",
    "        return country\n",
    "    \n",
    "def fix_diet(diet):\n",
    "    insectivore = diet == 'Insectivore' or diet == 'Carnivore, Insectivore'\n",
    "    piscivore = diet == 'Carnivore, Piscivore' or diet == 'Piscivore'\n",
    "    scavenger = diet == 'Carnivore, Scavenger'\n",
    "    filter_feeder = diet == 'Filter Feeder'\n",
    "    omnivore = diet == 'Herbivore, Omnivore' or diet == 'Omnivore, Herbivore'\n",
    "    insects = diet == 'Nectar, Insects' or diet == 'Insectivore, Herbivore' or diet == 'Omnivore, Insectivore'\n",
    "    \n",
    "    if insectivore or piscivore or scavenger or filter_feeder:\n",
    "        return 'Carnivore'\n",
    "    elif omnivore or insects:\n",
    "        return 'Omnivore'\n",
    "    else:\n",
    "        return diet \n",
    "\n",
    "def convert_diet(diet):\n",
    "    if diet == 'Carnivore':\n",
    "        return 0\n",
    "    elif diet == 'Omnivore':\n",
    "        return 1\n",
    "    elif diet == 'Herbivore':\n",
    "        return 2    \n",
    "\n",
    "new_dino = dinosaur.assign(\n",
    "    Diet = dinosaur['diet'].apply(format_diet),\n",
    "    Country = dinosaur['lived_in'],\n",
    "    Height = dinosaur['length'].apply(format_height)\n",
    ")\n",
    "\n",
    "final_dino = new_dino[['name', 'Diet','Country', 'Height', 'weight']]\n",
    "final_dino = final_dino.set_index('name')\n",
    "final_dino = final_dino.assign(Country=final_dino['Country'].apply(clean_countries))\n",
    "final_dino = final_dino.dropna()\n",
    "final_dino = final_dino.assign(Diet=final_dino['Diet'].apply(convert_diet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean animal DataFrame\n",
    "def convert_height(height): \n",
    "    return height / 100\n",
    "\n",
    "final_animal = animal[['Height', 'Diet', 'Animal', 'Countries Found', 'Weight']]\n",
    "\n",
    "final_animal = final_animal.assign(\n",
    "    Height = final_animal['Height'].apply(convert_height),\n",
    "    Country = final_animal['Countries Found']\n",
    ")\n",
    "\n",
    "final_animal = final_animal.set_index('Animal')\n",
    "\n",
    "final_animal = final_animal[['Diet', 'Country','Height', 'Weight']]\n",
    "final_animal = final_animal.assign(Diet=final_animal['Diet'].apply(fix_diet))\n",
    "final_animal = final_animal.assign(Diet=final_animal['Diet'].apply(convert_diet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(x, m):\n",
    "    return (0.5*m) / (1+np.exp(10/m * (0.5*m-x)))\n",
    "\n",
    "def find_difference(animal, feature1, feature2, index):\n",
    "    difference = np.abs(final_animal.loc[animal][feature1]-final_dino[feature2])\n",
    "    feature_mean = difference.mean()\n",
    "    delta_feature = np.abs((final_animal.loc[animal][feature1] - final_dino.iloc[index][feature2]))\n",
    "    out = ((delta_feature) / feature_mean) ** 2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Function\n",
    "def calculate_similarity(animal):\n",
    "    similarity_array = np.array([])\n",
    "    animal_location = final_animal.loc[animal]['Country']\n",
    "    distances_location = distances.reset_index()\n",
    "    distances_location = distances_location[distances_location['Country1'] == animal_location]\n",
    "    \n",
    "    for i in range(final_dino.shape[0]):\n",
    "        dist_sum = 0\n",
    "        #Diet Similarity\n",
    "        difference = np.abs(final_animal.loc[animal]['Diet']-final_dino['Diet'])\n",
    "        dist_sum += difference.iloc[i] ** 2\n",
    "             \n",
    "        #Height Similarity\n",
    "        dist_sum += find_difference(animal, 'Height', 'Height', i)\n",
    "        \n",
    "        \n",
    "        #Weight Similarity\n",
    "        dist_sum += find_difference(animal, 'Weight', 'weight', i)\n",
    "        \n",
    "        #Distance Similarity\n",
    "        delta_dist = distances_location[distances_location['Country2']==final_dino.iloc[i]['Country']]['Distance']\n",
    "        dist_mean = distances_location['Distance'].mean()\n",
    "\n",
    "        dist_sum += ((delta_dist) / dist_mean) ** 2\n",
    "        \n",
    "        similarity_score = dist_sum ** 0.5\n",
    "        similarity_array = np.append(similarity_array, similarity_score)\n",
    "    \n",
    "    updated_df = final_dino.assign(Similarity_Score=similarity_array)\n",
    "    return updated_df.sort_values('Similarity_Score', ascending=True).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions For Printing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stats(animal, dino):\n",
    "    animal_stats = final_animal[['Diet','Height','Weight','Country']].loc[animal]\n",
    "    dino_stats = final_dino[['Diet','Height','weight','Country']].loc[dino]\n",
    "    \n",
    "    convert_diet = ['Carnivore','Omnivore','Herbivore']\n",
    "    \n",
    "    # Compare height/weight:\n",
    "    height_string = animal + \"'s are approximately \"\\\n",
    "        + str(animal_stats['Height']) + \" meters tall \"\\\n",
    "        \"while \" + dino + \"'s were approximately \"\\\n",
    "        + str(dino_stats['Height']) + \" meters tall.\"\n",
    "    weight_string = animal + \"'s are approximately \"\\\n",
    "        + str(animal_stats['Weight']) + \" kilograms \"\\\n",
    "        \"while \" + dino + \"'s were approximately \"\\\n",
    "        + str(dino_stats['weight']) + \" kilograms.\"\n",
    "    if animal_stats['Diet'] == dino_stats['Diet']:\n",
    "        diet_string = animal + \"'s \" + dino + \"'s are/were both \"\\\n",
    "            + str(convert_diet[animal_stats['Diet']]) + \"s!\"\n",
    "    else:\n",
    "        diet_string = animal + \"'s are \" + animal_stats['Diet'] + \"'s while \"\\\n",
    "            + dino + \"'s were \" + convert_diet[dino_stats['Diet']] + \"s.\"\n",
    "    if animal_stats['Country'] == dino_stats['Country']:\n",
    "        country_string = animal + \"s and \" + dino + \"'s are/were both from \"\\\n",
    "            + animal_stats['Country'] + \"!\"\n",
    "    else:\n",
    "        country_string = animal + \"'s are from \" + animal_stats['Country'] + \" while \"\\\n",
    "            + dino + \"'s were from \" + dino_stats['Country'] + \".\"\n",
    "        \n",
    "    print(height_string)\n",
    "    print(weight_string)\n",
    "    print(diet_string)\n",
    "    print(country_string)\n",
    "    \n",
    "    \n",
    "def additional_features(dino):\n",
    "    output_units = ['years', 'km/hr', 'km/hr', 'days']\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f'Predicted {output}: {round(pred_stat(dino, output, network))} ' + output_units[i])\n",
    "\n",
    "\n",
    "def show_image(organism):\n",
    "    try:\n",
    "        display(Image.open(\"images1/images1/\"+ organism +\".png\"))\n",
    "    except:\n",
    "        display(Image.open(\"images2/images2/\"+ organism +\".png\"))    \n",
    "\n",
    "def transform_animal(animal):\n",
    "    most_similar = calculate_similarity(animal)\n",
    "    print(\"You put the following animal into the animal-dinosaur tranformer:\")\n",
    "    #time.sleep(1)\n",
    "    show_image(animal)\n",
    "    #time.sleep(1)\n",
    "    print(\"Say goodbye! Forever!\")\n",
    "    #time.sleep(1)\n",
    "    print(\"You shut the door and press the big red button\")\n",
    "    #time.sleep(1)\n",
    "    for i in range(3):\n",
    "        print(\"Transforming...\")\n",
    "        #time.sleep(1)\n",
    "    print(\"Something new walks out! It's different, yet somehow similar...\")\n",
    "    #time.sleep(1)\n",
    "    #similarities = calculate_similarity_rank(animal)\n",
    "    #name_of_most_similar = similarities.index[0]\n",
    "    print(\"Congrats!\")\n",
    "    #time.sleep(1)\n",
    "    print(\"You now have a \" + most_similar + \"!\")\n",
    "    #time.sleep(1)\n",
    "    print(\"Here's your new best friend!\")\n",
    "    #time.sleep(1)\n",
    "    show_image(most_similar)\n",
    "    compare_stats(animal, most_similar)\n",
    "    additional_features(most_similar)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropdown and Final Program\n",
    "\n",
    "### Choose Animal Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28327bdc372e4614b55abf52e52ebd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Animal', options=('Aardvark', 'Aardwolf', 'African Elephant', 'African Lion', 'African W…"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropdown = widgets.Dropdown(\n",
    "    options= animals['Animal'],\n",
    "    value='Aardvark',\n",
    "    description='Animal',\n",
    "    disabled=False,\n",
    ")\n",
    "        \n",
    "dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not found in database.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    transform_animal(dropdown.value)\n",
    "except:\n",
    "    print(\"Image not found in database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
